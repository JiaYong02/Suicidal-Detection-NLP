{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = open('./data/Data_1.txt','r')\n",
    "# print(data_1.read())\n",
    "str_1 = data_1.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "# Re Function\n",
    "import re\n",
    "pattern = r\"\\b\\w+(?:’\\w)?\\b|[.,:]\"\n",
    "tokens_regex = re.findall(pattern, str_1) # NOTE: check back this + this didn't capture stop words at all\n",
    "counter = 0\n",
    "for w in tokens_regex:\n",
    "    counter+=1\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split Function :  ['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.']\n",
      "\n",
      "Re Function :  ['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people’s', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n",
      "\n",
      "NLTK Function :  ['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', '’', 's', 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Original list\\n\",str_1)\n",
    "\n",
    "\"\"\"\n",
    "Textual information in the world can be broadly categorized into two main types: facts and opinions.\n",
    "Facts are objective expressions about entities, events, and their properties. Opinions are usually \n",
    "subjective expressions that describe :people's sentiments, appraisals, or feelings toward entities, events, and their properties.\n",
    "\"\"\"\n",
    "\n",
    "# Split function\n",
    "split_func = str_1.split()\n",
    "print(\"\\nSplit Function : \",split_func) # NOTE: This can't slit those stop words with the letters\n",
    "\n",
    "# Re Function\n",
    "import re\n",
    "pattern = r\"\\b\\w+(?:’\\w+)?\\b|[.,:]\"\n",
    "tokens_regex = re.findall(pattern, str_1) # NOTE: check back this + this didn't capture stop words at all\n",
    "print(\"\\nRe Function : \",tokens_regex)\n",
    "\n",
    "# NLTK Function\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "tokens_nltk = nltk.word_tokenize(str_1)\n",
    "print(\"\\nNLTK Function : \",tokens_nltk,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* Count *************************\n",
      "Cleaned Text:  29\n",
      "['Textual', 'information', 'world', 'broadly', 'categorized', 'two', 'main', 'types', 'facts', 'opinions', 'Facts', 'objective', 'expressions', 'entities', 'events', 'properties', 'Opinions', 'usually', 'subjective', 'expressions', 'describe', 'people’s', 'sentiments', 'appraisals', 'feelings', 'toward', 'entities', 'events', 'properties']\n",
      "Stop words:  15\n",
      "['in', 'the', 'can', 'be', 'into', 'and', 'are', 'about', 'and', 'their', 'are', 'that', 'or', 'and', 'their']\n",
      "Punctuations: 10\n",
      "[':', '.', ',', ',', '.', ',', ',', ',', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_punc_regex(text):\n",
    "    # Tokenize the text using the regular expression pattern\n",
    "    pattern = r\"\\b\\w+(?:’\\w)?\\b|[.,:]\"\n",
    "    tokens_regex = re.findall(pattern, text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [w for w in tokens_regex if w.lower() not in stop_words]\n",
    "    cap_stop_words = [w for w in tokens_regex if w.lower() in stop_words] \n",
    "    # Remove punc\n",
    "    text_clean_punc =[w for w in filtered_words if w not in string.punctuation]\n",
    "    cap_stop_punc  =[w for w in filtered_words if w in string.punctuation] \n",
    "    return text_clean_punc,cap_stop_words,cap_stop_punc\n",
    "\n",
    "cleaned_text_regex,stop_words,punc_sign = remove_stop_punc_regex(str_1)\n",
    "\n",
    "\n",
    "print('*'*25,'Count','*'*25)\n",
    "counter =0\n",
    "for w in cleaned_text_regex:\n",
    "    counter+=1\n",
    "print('Cleaned Text: ',counter)\n",
    "print(cleaned_text_regex)\n",
    "\n",
    "counter1 =0\n",
    "for w in stop_words:\n",
    "    counter1+=1\n",
    "print('Stop words: ',counter1)\n",
    "print(stop_words)\n",
    "\n",
    "counter2 =0\n",
    "for s in punc_sign:\n",
    "    counter2+=1\n",
    "print('Punctuations:',counter2)\n",
    "print(punc_sign)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_punc_regex(text):\n",
    "    # Tokenize the text using the regular expression pattern\n",
    "    pattern = r\"\\b\\w+(?:’\\w+)?\\b|[.,:]\"\n",
    "    tokens_regex = re.findall(pattern, text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [w for w in tokens_regex if w.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words into a clean text\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "\n",
    "    # Remove punctuations\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    text_no_punct = clean_text.translate(translator)\n",
    "\n",
    "    # Trim extra white space\n",
    "    text_no_punct = text_no_punct.replace('  ',' ')\n",
    "\n",
    "    return text_no_punct,stop_words,filtered_words\n",
    "\n",
    "\n",
    "# Remove stop words and punctuation using regex-based tokenization\n",
    "cleaned_text_regex,stop_words = remove_stop_punc_regex(str_1)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(\"Cleaned Text (Regex Tokenization):\")\n",
    "print(cleaned_text_regex)\n",
    "\n",
    "\n",
    "# print(f\"\\nRemoved Stop words: \\n{stop_words}\")\n",
    "print('Stop words:\\n')\n",
    "for w in stop_words:\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence,', 'with', 'some', 'punctuation!']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is an example sentence, with some punctuation!\"\n",
    "tokens = text.split()  # Assuming you have tokenized the text\n",
    "\n",
    "tokens_without_punctuation = remove_punctuation(tokens)\n",
    "print(tokens_without_punctuation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txsa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
