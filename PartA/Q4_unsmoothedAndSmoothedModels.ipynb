{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "# from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'read', 'a', 'book', 'by', 'danielle']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "text = \"I read a book by Danielle\"\n",
    "tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'read'),\n",
       " ('read', 'a'),\n",
       " ('a', 'book'),\n",
       " ('book', 'by'),\n",
       " ('by', 'danielle')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'read'),\n",
       " ('read', 'a'),\n",
       " ('a', 'book'),\n",
       " ('book', 'by'),\n",
       " ('by', 'danielle')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, n=2)) # n = no of grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i',),\n",
       " ('i', 'read'),\n",
       " ('i', 'read', 'a'),\n",
       " ('read',),\n",
       " ('read', 'a'),\n",
       " ('read', 'a', 'book'),\n",
       " ('a',),\n",
       " ('a', 'book'),\n",
       " ('a', 'book', 'by'),\n",
       " ('book',),\n",
       " ('book', 'by'),\n",
       " ('book', 'by', 'danielle'),\n",
       " ('by',),\n",
       " ('by', 'danielle'),\n",
       " ('danielle',)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(everygrams(tokens, max_len=3)) # max_len will set the no of maximum grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'i', 'read', 'a', 'book', 'by', 'danielle', '</s>']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "list(pad_sequence(tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
    "# The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'i'),\n",
       " ('i', 'read'),\n",
       " ('read', 'a'),\n",
       " ('a', 'book'),\n",
       " ('book', 'by'),\n",
       " ('by', 'danielle'),\n",
       " ('danielle', '</s>')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sent = list(pad_sequence(tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
    "list(ngrams(padded_sent, n=2)) # bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'i', 'read', 'a', 'book', 'by', 'danielle', '</s>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "list(pad_both_ends(tokens, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'i'),\n",
       " ('i', 'read'),\n",
       " ('read', 'a'),\n",
       " ('a', 'book'),\n",
       " ('book', 'by'),\n",
       " ('by', 'danielle'),\n",
       " ('danielle', '</s>')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(pad_both_ends(tokens, n=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('i',),\n",
       " ('read',),\n",
       " ('a',),\n",
       " ('book',),\n",
       " ('by',),\n",
       " ('danielle',),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "padded_bigrams = list(pad_both_ends(tokens, n=2))\n",
    "list(everygrams(padded_bigrams, max_len=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'i'),\n",
       " ('i',),\n",
       " ('i', 'read'),\n",
       " ('read',),\n",
       " ('read', 'a'),\n",
       " ('a',),\n",
       " ('a', 'book'),\n",
       " ('book',),\n",
       " ('book', 'by'),\n",
       " ('by',),\n",
       " ('by', 'danielle'),\n",
       " ('danielle',),\n",
       " ('danielle', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(everygrams(padded_bigrams, max_len=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'read', 'a', 'book', 'by', 'danielle']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text = \"I read a book by Danielle\"\n",
    "# Tokenize the text.\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(text)))]\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE # Maximum Likelihood Estimation\n",
    "\n",
    "n = 2 # No of grams\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "model = MLE(n) # Lets train a 3-grams maximum likelihood estimation model.\n",
    "model.fit(train_data, padded_sents) # model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.lm.models.MLE at 0x26e2c2c8b90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsmoothed Bigram model (Method 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the sentence: 0.07407407407407407\n"
     ]
    }
   ],
   "source": [
    "#Function of calculating bigram probability\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram, 0))/(unigramCounts.get(word1, 0))\n",
    "    return listOfProb\n",
    "\n",
    "# Given corpus\n",
    "corpus = [\n",
    "    \"<s> He read a book </s>\",\n",
    "    \"<s> I read a different book </s>\",\n",
    "    \"<s> He read a book by Danielle </s>\"\n",
    "]\n",
    "\n",
    "# Function to extract bigrams from a list of sentences\n",
    "def extract_bigrams(sentences):\n",
    "    bigrams = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()  # Include <s> and </s> tokens\n",
    "        for i in range(len(words) - 1):\n",
    "            bigrams.append((words[i], words[i+1]))\n",
    "    return bigrams\n",
    "\n",
    "# Count unigrams and bigrams\n",
    "unigrams = [word for sentence in corpus for word in sentence.split()]\n",
    "bigrams = extract_bigrams(corpus)\n",
    "\n",
    "unigram_counts = {word: unigrams.count(word) for word in unigrams}\n",
    "bigram_counts = {bigram: bigrams.count(bigram) for bigram in bigrams}\n",
    "\n",
    "# Sentence for which we want to calculate the probability\n",
    "sentence = \"<s> I read a book by Danielle </s>\"\n",
    "sentence_bigrams = extract_bigrams([sentence])\n",
    "\n",
    "# Calculate the probability using the provided function\n",
    "bigram_probabilities = calcBigramProb(sentence_bigrams, unigram_counts, bigram_counts)\n",
    "\n",
    "# Compute the probability of the sentence\n",
    "sentence_probability = 1\n",
    "for bigram in sentence_bigrams:\n",
    "    sentence_probability *= bigram_probabilities.get(bigram, 0)\n",
    "\n",
    "# Print the probability\n",
    "print(\"Probability of the sentence:\", sentence_probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsmoothed Bigram model (Method 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the sentence: 0.07407407407407407\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate bigram probability\n",
    "def bigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram, 0))/(unigramCounts.get(word1, 0))\n",
    "    return listOfProb\n",
    "\n",
    "# Function to read corpus from a text file and filter specific lines\n",
    "def read_and_filter_corpus(file_path, lines_to_include):\n",
    "    with open(file_path, 'r') as file:\n",
    "        corpus = file.readlines()\n",
    "    filtered_corpus = [corpus[line_number] for line_number in lines_to_include] \n",
    "    return filtered_corpus\n",
    "\n",
    "# Function to extract bigrams from a list of sentences\n",
    "def extract_bigrams(sentences):\n",
    "    bigrams = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()  # This is to include <s> and </s> tokens\n",
    "        for i in range(len(words) - 1):\n",
    "            bigrams.append((words[i], words[i+1]))\n",
    "    return bigrams\n",
    "\n",
    "# Count unigrams and bigrams\n",
    "corpus_file_path = \"Data_3.txt\"\n",
    "lines_to_include = [2, 3, 4]  # Lines to include from the file (Training Corpus)\n",
    "corpus = read_and_filter_corpus(corpus_file_path, lines_to_include)\n",
    "\n",
    "unigrams = [word for sentence in corpus for word in sentence.split()]\n",
    "bigrams = extract_bigrams(corpus)\n",
    "\n",
    "unigram_counts = {word: unigrams.count(word) for word in unigrams}\n",
    "bigram_counts = {bigram: bigrams.count(bigram) for bigram in bigrams}\n",
    "\n",
    "# Calculate the probability of the sentence below\n",
    "sentence = \"<s> I read a book by Danielle </s>\"\n",
    "sentence_bigrams = extract_bigrams([sentence])\n",
    "\n",
    "# Calculate the probability using the function created above\n",
    "bigram_probabilities = bigramProb(sentence_bigrams, unigram_counts, bigram_counts)\n",
    "\n",
    "# Compute the probability of the sentence\n",
    "sentence_probability = 1\n",
    "for bigram in sentence_bigrams:\n",
    "    sentence_probability *= bigram_probabilities.get(bigram, 0)\n",
    "\n",
    "# Print the probability\n",
    "print(\"Probability of the sentence:\", sentence_probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothed Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the sentence: 1.0101357919757919e-05\n"
     ]
    }
   ],
   "source": [
    "def smoothedBigramProb(listOfBigrams, unigramCounts, bigramCounts, vocabulary_size):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        count_bigram = bigramCounts.get(bigram, 0)\n",
    "        count_unigram = unigramCounts.get(word1, 0)\n",
    "        # Apply Laplace smoothing\n",
    "        prob = (count_bigram + 1) / (count_unigram + vocabulary_size)\n",
    "        listOfProb[bigram] = prob\n",
    "    return listOfProb\n",
    "\n",
    "# Training corpus\n",
    "corpus = [\n",
    "    \"<s> He read a book </s>\",\n",
    "    \"<s> I read a different book </s>\",\n",
    "    \"<s> He read a book by Danielle </s>\"\n",
    "]\n",
    "\n",
    "# Function to extract bigrams from a list of sentences\n",
    "def extract_bigrams(sentences):\n",
    "    bigrams = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()  # to include <s> and </s> tokens\n",
    "        for i in range(len(words) - 1):\n",
    "            bigrams.append((words[i], words[i+1]))\n",
    "    return bigrams\n",
    "\n",
    "# Count unigrams and bigrams\n",
    "unigrams = [word for sentence in corpus for word in sentence.split()]\n",
    "bigrams = extract_bigrams(corpus)\n",
    "vocabulary_size = len(set(unigrams))  # Vocabulary size for Laplace smoothing , in this case its 10\n",
    "\n",
    "unigram_counts = {word: unigrams.count(word) for word in unigrams}\n",
    "bigram_counts = {bigram: bigrams.count(bigram) for bigram in bigrams}\n",
    "\n",
    "# Sentence for which we want to calculate the probability\n",
    "sentence = \"<s> I read a book by Danielle </s>\"\n",
    "sentence_bigrams = extract_bigrams([sentence])\n",
    "\n",
    "# Calculate the smoothed probability using the provided function\n",
    "smoothed_bigram_probabilities = smoothedBigramProb(sentence_bigrams, unigram_counts, bigram_counts, vocabulary_size)\n",
    "\n",
    "# Compute the probability of the sentence\n",
    "sentence_probability = 1\n",
    "for bigram in sentence_bigrams:\n",
    "    sentence_probability *= smoothed_bigram_probabilities.get(bigram, 1/(vocabulary_size))  # Default to 1/V if bigram not found\n",
    "\n",
    "# Print the probability\n",
    "print(\"Probability of the sentence:\", sentence_probability)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
