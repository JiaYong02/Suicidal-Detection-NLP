{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read csv file\n",
    "df = pd.read_csv('Suicide_Detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text        class\n",
      "0           2  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
      "1           3  Am I weird I don't get affected by compliments...  non-suicide\n",
      "2           4  Finally 2020 is almost over... So I can never ...  non-suicide\n",
      "3           8          i need helpjust help me im crying so hard      suicide\n",
      "4           9  Iâ€™m so lostHello, my name is Adam (16) and Iâ€™v...      suicide\n"
     ]
    }
   ],
   "source": [
    "# display firt 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0                                               text  \\\n",
      "232069      348103  If you don't like rock then your not going to ...   \n",
      "232070      348106  You how you can tell i have so many friends an...   \n",
      "232071      348107  pee probably tastes like salty teaðŸ˜ðŸ’¦â€¼ï¸ can som...   \n",
      "232072      348108  The usual stuff you find hereI'm not posting t...   \n",
      "232073      348110  I still haven't beaten the first boss in Hollo...   \n",
      "\n",
      "              class  \n",
      "232069  non-suicide  \n",
      "232070  non-suicide  \n",
      "232071  non-suicide  \n",
      "232072      suicide  \n",
      "232073  non-suicide  \n"
     ]
    }
   ],
   "source": [
    "# display last 5 rows\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(232074, 3)\n"
     ]
    }
   ],
   "source": [
    "# Get the dimensions \n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "text          0\n",
      "class         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for any missing values \n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0     int64\n",
      "text          object\n",
      "class         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# data types \n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate data found in the 'text' column.\n"
     ]
    }
   ],
   "source": [
    "# check duplicate\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"Suicide_Detection.csv\")\n",
    "\n",
    "# Check for duplicate data in the \"text\" column\n",
    "duplicate_rows = df[df.duplicated(['text'], keep=False)]\n",
    "\n",
    "# Print duplicate rows\n",
    "if len(duplicate_rows) > 0:\n",
    "    print(\"Duplicate data found in the following rows:\")\n",
    "    print(duplicate_rows)\n",
    "else:\n",
    "    print(\"No duplicate data found in the 'text' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ChokJoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# read csv file\n",
    "df = pd.read_csv(\"Suicide_Detection.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# drop 1st column as it is integer  \n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# remove n/a\n",
    "def remove_missing_values(df):\n",
    "    return df.dropna(subset=['text'])\n",
    "\n",
    "# remove duplicate data\n",
    "def remove_duplicates(df):\n",
    "    return df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# lowercase the text data\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# remove symbols\n",
    "def remove_symbols(text):\n",
    "    symbol_pattern = re.compile(r'[\\(\\)\\[\\]:]')\n",
    "    return symbol_pattern.sub('', text)\n",
    "\n",
    "# remove symbols and digits\n",
    "def remove_symbols_digits(text):\n",
    "    return re.sub('[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "# remove emoji\n",
    "def remove_emoji(text):\n",
    "    return re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001FB00-\\U0001FBFF\\U0001FE00-\\U0001FE0F\\U0001F004]+', '', text)\n",
    "\n",
    "# remove URLs\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "# remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "# remove extra whitespace\n",
    "def remove_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# expand the contractions of text ,e.g., cnnt convert to cannot\n",
    "def expand_contractions(text):\n",
    "\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "# # remove punctuation\n",
    "# def remove_punctuation(text):\n",
    "#     return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# # remove stopwords\n",
    "# def remove_stopwords(text):\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     return ' '.join([token for token in text.split() if token.lower() not in stop_words])\n",
    "\n",
    "# # stemming text\n",
    "# def stem_text(text):\n",
    "#     stemmer = PorterStemmer()\n",
    "#     return ' '.join([stemmer.stem(token) for token in text.split()])\n",
    "\n",
    "# # lemmatizing text\n",
    "# def lemmatize_text(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     return ' '.join([lemmatizer.lemmatize(token) for token in text.split()])\n",
    "\n",
    "# # tokenize the text data\n",
    "# def tokenize_text(text):\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "# Apply preprocessing techniques sequentially\n",
    "df = remove_missing_values(df)\n",
    "df = remove_duplicates(df)\n",
    "df['text'] = df['text'].apply(lowercase_text)\n",
    "df['text'] = df['text'].apply(remove_symbols)\n",
    "df['text'] = df['text'].apply(remove_symbols_digits)\n",
    "df['text'] = df['text'].apply(remove_emoji)\n",
    "df['text'] = df['text'].apply(remove_urls)\n",
    "df['text'] = df['text'].apply(remove_html_tags)\n",
    "df['text'] = df['text'].apply(remove_whitespace)\n",
    "df['text'] = df['text'].apply(expand_contractions)\n",
    "# df['text'] = df['text'].apply(remove_punctuation)\n",
    "# df['text'] = df['text'].apply(remove_stopwords)\n",
    "# df['text'] = df['text'].apply(stem_text)\n",
    "# df['text'] = df['text'].apply(lemmatize_text)\n",
    "# df['text'] = df['text'].apply(tokenize_text)\n",
    "\n",
    "\n",
    "\n",
    "# Save the preprocessed DataFrame to a new CSV file\n",
    "df.to_csv(\"cleaned_suicide_detection.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
