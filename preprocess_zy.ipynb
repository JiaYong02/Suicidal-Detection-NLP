{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0K+2Glqv05Qf5Cb29yoqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaYong02/Suicidal-Detection-NLP/blob/main/preprocess_zy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import"
      ],
      "metadata": {
        "id": "FA56-nqzJai9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ZuXwu5XiJWbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre-processing\n",
        "- **Take notes**: dataset used in this section should be untokenized -> refer joe zip files"
      ],
      "metadata": {
        "id": "zRBRLgw9JgZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cln = pd.read_csv('/content/drive/MyDrive/Colab-Notebooks/txsa/cleaned_SuicideAndDepression_detection.csv')\n",
        "len(df_cln)"
      ],
      "metadata": {
        "id": "xSZ_oHGtJfS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop nan"
      ],
      "metadata": {
        "id": "93q59mCGJ5kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop na\n",
        "df_cln = df_cln.dropna()"
      ],
      "metadata": {
        "id": "K3wECHkPJlyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization Process\n",
        "- Take notes: this process should be carry out befor doing non-english words removal"
      ],
      "metadata": {
        "id": "TvdXYY2XJnxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return  word_tokenize(text)\n",
        "\n",
        "# Apply tokenization to the text column in the DataFrame\n",
        "df_cln['text'] = df_cln['text'].apply(tokenize_text)"
      ],
      "metadata": {
        "id": "O5Yp3tRsJxKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-english words removal"
      ],
      "metadata": {
        "id": "-u-8EeABJ2Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove english words\n",
        "\n",
        "from nltk.corpus import words\n",
        "import nltk\n",
        "\n",
        "# Download the list of English words (if not already downloaded)\n",
        "nltk.download('words')\n",
        "\n",
        "# Load the set of English words\n",
        "english_words = set(words.words())\n",
        "\n",
        "# List of words to exclude from removal\n",
        "words_to_exclude = {'fuck'}  # Add your specific words here\n",
        "\n",
        "# Function to remove non-English words from a list of tokens\n",
        "def remove_non_english(tokens):\n",
        "    english_tokens = [\n",
        "        token if (token in english_words or token in words_to_exclude) else ''\n",
        "        for token in tokens]\n",
        "    return [token for token in english_tokens if token != '']\n",
        "\n",
        "# Apply the function to the 'text' column in the DataFrame\n",
        "df_cln['text'] = df_cln['text'].apply(remove_non_english)"
      ],
      "metadata": {
        "id": "rs9ip-ilJ1Gw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}